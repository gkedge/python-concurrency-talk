{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attempted-purchase",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Concurrency: \n",
    "## Threads, Processes & AsyncIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-canadian",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theee Dreaded *Global Interpreter Lock (GIL)!*\n",
    "CPython's limits the intretation of Python source to a single thread.\n",
    "\n",
    "GIL released when not interpreting Python source:\n",
    "- Sleep\n",
    "- I/O\n",
    "- Preemptively in 15 milliseconds in Python 3\n",
    "- *That* bold C/C++ extensions author *can* release and reacquire the GIL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-powder",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is not uncommmon for other, even \"high-level\" languages, where\n",
    "- *IF* code processing is CPU-intensive AND\n",
    "- *IF* processing algorithm is distributable  \n",
    "\n",
    "... threading can leverage available CPU cores to divide and conqure.\n",
    "\n",
    "Running distributable CPU-bound algoritms using Python threads will run slower than running to completion on a single thread.\n",
    "\n",
    "**Thanks GIL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-anchor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPU-Bound Serial vs Threaded Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-london",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in: \n",
      "  [0]: 0:00:00.777690\n",
      "  [1]: 0:00:01.527962\n",
      "  [2]: 0:00:02.271462\n",
      "  [3]: 0:00:03.087205\n",
      "  [4]: 0:00:03.857064\n",
      "  [5]: 0:00:04.660319\n",
      "  [6]: 0:00:05.463782\n",
      "  [7]: 0:00:06.295601\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cpus = os.cpu_count()\n",
    "start = datetime.now()\n",
    "print(\"Finished in: \")\n",
    "for n in range(cpus):\n",
    "    pow(42, 1_250_000)\n",
    "    print(f\"  [{n}]: {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-vaccine",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding threads are very easy to introduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-egyptian",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in: \n",
      "  [0]: 0:00:06.463699\n",
      "  [1]: 0:00:06.463851\n",
      "  [2]: 0:00:06.463878\n",
      "  [3]: 0:00:06.463899\n",
      "  [4]: 0:00:06.463919\n",
      "  [5]: 0:00:06.463935\n",
      "  [6]: 0:00:06.463954\n",
      "  [7]: 0:00:06.463973\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "start = datetime.now()\n",
    "\n",
    "print(\"Finished in: \")\n",
    "with ThreadPoolExecutor(max_workers=cpus) as executor:\n",
    "    futures = {executor.submit(pow, 42, 1_250_000): n for n in range(cpus)}\n",
    "    for n, fut in enumerate(as_completed(futures)):\n",
    "        print(f\"  [{n}]: {datetime.now() - start}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-owner",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### ... not so gud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-pillow",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yet, threads are:\n",
    "- difficult to guard shared state along all execution paths\n",
    "- difficult to reason: read, maintain, grok\n",
    "- brittle to change\n",
    "- fuzzing can discover thread-safety weaknesses but labor intensive and still incomplete an complete\n",
    "\n",
    "Failures that smell like incorrect thread-safety are hard to reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-female",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Fuzzing can ferret out thread-safety weaknesses but that is very labor intensive and really, it wouldn't   to test for complete thread-safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-bradley",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Would Python have the popularity it enjoys w/o The GIL?\n",
    "\n",
    "What significant community hates the GIL?\n",
    "\n",
    "Threads only exist in Python to optimize sets of independent I/O tasks that can be performed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-dryer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I/O-bound Threaded Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "early-motivation",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in:\n",
      "  [0]:    p785.pdf[    19,111] 0:00:01.377155\n",
      "  [1]:    p487.pdf[    22,893] 0:00:01.431402\n",
      "  [2]: f1040sb.pdf[    71,970] 0:00:01.462310\n",
      "  [3]:   f1040.pdf[   150,184] 0:00:01.479610\n",
      "  [4]:   p5384.pdf[52,350,859] 0:00:06.516602\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "urls = [ \"f1040.pdf\",\"f1040sb.pdf\",\"p785.pdf\",\"p487.pdf\",\"p5384.pdf\" ]\n",
    "start = datetime.now()\n",
    "\n",
    "def requests_url(url):\n",
    "    response = requests.get(\"https://www.irs.gov/pub/irs-pdf/\" + url)\n",
    "    num_str = f'{len(response.content):,}'\n",
    "    return f'{num_str:>10}'\n",
    "\n",
    "print(\"Finished in:\")\n",
    "with ThreadPoolExecutor(max_workers=cpus) as executor:\n",
    "    futures = {executor.submit(requests_url, url): url for url in urls }\n",
    "    for n, fut in enumerate(as_completed(futures)):        \n",
    "        print(f\"  [{n}]: {futures[fut]:>11}[{fut.result()}] {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-discovery",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Converting Threading to Processes? Wow.  So, Easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aboriginal-darkness",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in: \n",
      "  [0]: 0:00:01.474591\n",
      "  [1]: 0:00:01.482764\n",
      "  [2]: 0:00:01.486232\n",
      "  [3]: 0:00:01.489614\n",
      "  [4]: 0:00:01.493342\n",
      "  [5]: 0:00:01.496679\n",
      "  [6]: 0:00:01.499951\n",
      "  [7]: 0:00:01.556065\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "start = datetime.now()\n",
    "\n",
    "print(\"Finished in: \")\n",
    "with ProcessPoolExecutor(max_workers=cpus) as executor:\n",
    "    futures = {executor.submit(pow, 42, 1_250_000): n for n in range(cpus)}\n",
    "    for n, fut in enumerate(as_completed(futures)):\n",
    "        print(f\"  [{n}]: {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-airfare",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yet, processes need RPC and data serialization to share:\n",
    "- input\n",
    "- output\n",
    "- errors\n",
    "- state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-wallpaper",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible to realize concurrent I/O without threads?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-webmaster",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Possible to realize concurrent I/O without threads?\n",
    "#### Yes.  Twisted python package pulled this off, but Guido wanted to build AsyncIO into language.\n",
    "##### Oooh... new keywords: `async` and `await`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-roller",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O-Bound Execution Threaded vs AsyncIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "final-accounting",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def threaded_io():\n",
    "    start = datetime.now()\n",
    "    print(\"Threaded:\")\n",
    "    with ThreadPoolExecutor(max_workers=cpus) as executor:\n",
    "        futures = {executor.submit(requests_url, url): url for url in urls }\n",
    "        for n, fut in enumerate(as_completed(futures)):        \n",
    "            print(f\"  [{n}]: {futures[fut]:>11}[{fut.result()}] {datetime.now() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility-budapest",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threaded:\n",
      "  [0]:    p785.pdf[    19,111] 0:00:00.339049\n",
      "  [1]:    p487.pdf[    22,893] 0:00:00.339216\n",
      "  [2]:   f1040.pdf[   150,184] 0:00:00.360534\n",
      "  [3]: f1040sb.pdf[    71,970] 0:00:00.366088\n",
      "  [4]:   p5384.pdf[52,350,859] 0:00:05.647209\n",
      "AsyncIO:\n",
      "  [0]:   f1040.pdf[   150,184] 0:00:00.383307\n",
      "  [1]: f1040sb.pdf[    71,970] 0:00:00.453102\n",
      "  [2]:    p785.pdf[    19,111] 0:00:00.482272\n",
      "  [3]:    p487.pdf[    22,893] 0:00:00.611930\n",
      "  [4]:   p5384.pdf[52,350,859] 0:00:05.986217\n"
     ]
    }
   ],
   "source": [
    "threaded_io()\n",
    "\n",
    "import aiohttp; import asyncio; import nest_asyncio; from datetime import datetime\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def load_urls():\n",
    "    n = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(\"AsyncIO:\"); start = datetime.now()\n",
    "        for url in urls:\n",
    "            async with session.get(\"https://www.irs.gov/pub/irs-pdf/\" + url) as resp:\n",
    "                content = await resp.read()\n",
    "                file_size = f\"{len(content):,}\"\n",
    "                print(f\"  [{n}]: {url:>11}[{file_size:>10}] {datetime.now() - start}\"); n += 1\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(load_urls())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-suspect",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Q: What be this async magic?\n",
    "A: Coopertative mult-tasking\n",
    "\n",
    "What is old is new.\n",
    "\n",
    "Q: How?\n",
    "A: At the heart of any `await()` is a `yeild` of `generator` fame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-ethiopia",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AsyncIO's Attractions\n",
    "- No locks! So, much easier to get your code correct.\n",
    "- Switching from tasks cooperatively using AsyncIO is _faster_ than calling a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-european",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AsyncIO's Detractions\n",
    "- Breaks abstractions, meaning: hard to reason `goto` behavior\n",
    "- Breaks `with` statements\n",
    "- Breaks exceptions\n",
    "- Any blocking (I/O, time.sleep(), etc.) compromises AysncIO's benefits; async versions of blocking methods must be leveraged\n",
    "- Enormous learning curve to understand/debug the Async loop and numerous replacements for blocking I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-danish",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Next Level: Co-routines + Threads\n",
    "### Nathanial Smith's Trio Package\n",
    "Makes asyncio easier to implement, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-bryan",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alex Martelli Model of Scalability\n",
    "- 1 core: single thread and single process\n",
    "- 2-8 cores: multiple threads and multiple processes\n",
    "  - GPUs for limited computations and low-level languages\n",
    "- 9+ cores: distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-kingston",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Martelli was part of early Google and made the following observations:\n",
    "- Single threaded scaling due to the constant improvements of the speed of single core CPU (Moore's Law)\n",
    "  MANY problems can be solved with the massive CPU power that can be purchased.\n",
    "- 2-N cores (augemented w/ cache trashing hyper-threading) can combine with threaded programming to perform parallel operations on separate data.  \n",
    "  If your problem can be solved with N cores, great. But, your problem is pracariously close to reaching the hard limits for massively parallel operations.  \n",
    "  Even when considering GPU, there are limits to what kind of computations and data sizes that can be addressed using low-level CUDA.\n",
    "\n",
    "So, there are good reasons to consider the diminishing return of leveraging parallelization through threading and skip right to distributed computing; essentially the Kubernetes' focus to leverage CPU cores at a higher level further managing failures, state and data using distribution designs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-organ",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommendations\n",
    "Raymond Hettinger's Concurrency Keynote PyBay 2017  \n",
    "\n",
    "Nathaniel J. Smith'a Trio: Async concurrency for mere mortals - PyCon 2018"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}